{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557db926-eb60-4b91-bd28-36f8f66fab66",
   "metadata": {},
   "source": [
    "# Notebook Overview: Tuning for HAN-GRU Model\n",
    "\n",
    "This model is same as the model introduced in 3.3.2 in the project report, but instead is trained with the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2253be-8701-4a8a-9653-321d0321b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512267cf-0847-4645-9022-cd52919db375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/lyrics_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768a02cf-6189-4b72-9cd3-59ac01e6da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 218162/218162 [00:24<00:00, 8861.79it/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in tqdm(df['lyrics']):\n",
    "    counter.update(tokenizer(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f1e775e-31ba-46d1-a802-d888cb26604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary using build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator([tokenizer(line) for line in df['lyrics']],\n",
    "                                  specials=['<unk>', '<pad>'], min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af568621-b2cb-43c7-a1cd-573f62bba6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "indexed_data = [torch.tensor([vocab[token] for token in tokenizer(line)])\n",
    "                for line in df['lyrics']]\n",
    "\n",
    "# Include padding for same shape size\n",
    "max_seq_length = max(len(seq) for seq in indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c4dd07-d21a-422a-86a5-4cac5aa650fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = pad_sequence(indexed_data, batch_first=True, padding_value=vocab['<pad>'])\n",
    "indexed_labels = torch.tensor(label_encoder.fit_transform(df['genre']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b750e1b7-8828-4332-adec-f8468f3aff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174,529 training samples\n",
      "43,633 validation samples\n"
     ]
    }
   ],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, lyrics, genre):\n",
    "        self.lyrics = lyrics\n",
    "        self.genre = genre\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.genre)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lyrics[idx], self.genre[idx]\n",
    "\n",
    "dataset = LyricsDataset(padded_data, indexed_labels)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d009163d-5b49-4e0d-9f12-6c28dfcd127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c225a9-57ca-4058-a64a-9c54f3a27f7b",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee8a0a4-158d-458e-b2a4-ce272aa99334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(AttLayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = nn.Parameter(torch.randn(input_size, hidden_dim))\n",
    "        self.bw = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.uw = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_words, hidden_size = x.size()\n",
    "        x_reshaped = x.reshape(-1, hidden_size)\n",
    "\n",
    "        ui = torch.tanh(torch.matmul(x_reshaped, self.W) + self.bw)\n",
    "        intermed = torch.sum(self.uw * ui, dim=1)\n",
    "\n",
    "        intermed = intermed.view(batch_size, num_words)\n",
    "        weights = torch.softmax(intermed, dim=-1)\n",
    "        weights = weights.unsqueeze(-1)\n",
    "\n",
    "        weighted_input = x * weights\n",
    "        return torch.sum(weighted_input, dim=1)\n",
    "\n",
    "class HAN_GRU(nn.Module):\n",
    "    def __init__(self, num_words, embedding_vector_length, hidden_size, attention_size, max_words_per_line, max_num_lines, output_size):\n",
    "        super(HAN_GRU, self).__init__()\n",
    "\n",
    "        self.word_embedding = nn.Embedding(num_words, embedding_vector_length)\n",
    "        self.word_gru = nn.GRU(embedding_vector_length, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.word_attention = AttLayer(hidden_size * 2, attention_size)\n",
    "\n",
    "        self.sentence_gru = nn.GRU(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.sentence_attention = AttLayer(hidden_size * 2, attention_size)\n",
    "\n",
    "        self.max_words_per_line = max_words_per_line\n",
    "        self.max_num_lines = max_num_lines\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        word_embedded = self.word_embedding(inputs)\n",
    "\n",
    "\n",
    "        word_output, _ = self.word_gru(word_embedded)\n",
    "        word_attention_output = self.word_attention(word_output)\n",
    "\n",
    "        batch_size = word_attention_output.size(0)\n",
    "        sentence_input = word_attention_output.view(batch_size, -1, word_attention_output.size(-1))\n",
    "\n",
    "        sentence_output, _ = self.sentence_gru(sentence_input)\n",
    "        sentence_attention_output = self.sentence_attention(sentence_output)\n",
    "\n",
    "        \n",
    "        document_output = sentence_attention_output.view(batch_size, -1)\n",
    "        output = self.fc(self.dropout(document_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f612658-3bad-4460-a45e-b535f13abaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Words Per Line: 6232\n",
      "Max Number of Lines: 759\n"
     ]
    }
   ],
   "source": [
    "# Calculate max words per line and max number of lines\n",
    "max_words_per_line = df['lyrics'].apply(lambda x: len(x.split())).max()\n",
    "max_num_lines = df['lyrics'].apply(lambda x: len(x.split('\\n'))).max()\n",
    "print(f\"Max Words Per Line: {max_words_per_line}\")\n",
    "print(f\"Max Number of Lines: {max_num_lines}\")\n",
    "\n",
    "attention_size = 100\n",
    "hidden_size = 128\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 512 \n",
    "output_size = len(df['genre'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d5fe08f-e184-462a-ace2-a72631b9f1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "\n",
      "HAN_GRU(\n",
      "  (word_embedding): Embedding(245576, 512)\n",
      "  (word_gru): GRU(512, 128, batch_first=True, bidirectional=True)\n",
      "  (word_attention): AttLayer()\n",
      "  (sentence_gru): GRU(256, 128, batch_first=True, bidirectional=True)\n",
      "  (sentence_attention): AttLayer()\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print()\n",
    "model = HAN_GRU(vocab_size, embedding_dim, hidden_size, attention_size,\n",
    "                max_words_per_line, max_num_lines, output_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "812d4fd0-1543-45f6-a929-5ed325a6f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [10:56<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Train Loss: 1.4289 Train Accuracy: 54.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [10:56<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] Train Loss: 1.3877 Train Accuracy: 55.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [10:56<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] Train Loss: 1.3512 Train Accuracy: 56.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [10:56<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] Train Loss: 1.3119 Train Accuracy: 58.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [10:56<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] Train Loss: 1.2603 Train Accuracy: 59.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [10:57<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] Train Loss: 1.1849 Train Accuracy: 62.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [10:56<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] Train Loss: 1.1138 Train Accuracy: 64.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [10:56<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] Train Loss: 1.0387 Train Accuracy: 66.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [10:54<00:00,  8.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] Train Loss: 0.9602 Train Accuracy: 69.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [10:54<00:00,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] Train Loss: 0.8840 Train Accuracy: 71.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses, accuracies = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_accuracy = (correct_predictions / total_predictions) * 100\n",
    "    losses.append(epoch_loss)\n",
    "    accuracies.append(epoch_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {epoch_loss:.4f} Train Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66000c9f-2804-4f7d-a0d6-350461a9b17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.3125, Accuracy: 59.98%\n"
     ]
    }
   ],
   "source": [
    "# Validation loop (optional)\n",
    "model.eval()\n",
    "val_running_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in validation_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        val_loss = criterion(outputs, labels)\n",
    "        val_running_loss += val_loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    avg_val_loss = val_running_loss / len(validation_dataloader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93b9607-7d3a-44ee-afc8-c5bdf4cf8456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
