{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557db926-eb60-4b91-bd28-36f8f66fab66",
   "metadata": {},
   "source": [
    "# Notebook Overview: Tuning for HAN-GRU Model #2\n",
    "\n",
    "We aimed to enhance the model's performance by modifying specific hyperparameters. We increased the attention size from 100 to 200 to enable a more comprehensive capture of intricate lyrical relationships. However, this adjustment posed a potential risk of overfitting. To counter this, we raised the dropout rate from 0.3 to 0.4, mitigating the risk of overfitting by introducing more regularization. Additionally, we reduced the hidden size to encourage the model to generalize better on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c2253be-8701-4a8a-9653-321d0321b654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab, build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512267cf-0847-4645-9022-cd52919db375",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/lyrics_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768a02cf-6189-4b72-9cd3-59ac01e6da1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 218162/218162 [00:27<00:00, 7959.63it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in tqdm(df['lyrics']):\n",
    "    counter.update(tokenizer(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f1e775e-31ba-46d1-a802-d888cb26604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vocabulary using build_vocab_from_iterator\n",
    "vocab = build_vocab_from_iterator([tokenizer(line) for line in df['lyrics']],\n",
    "                                  specials=['<unk>', '<pad>'], min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af568621-b2cb-43c7-a1cd-573f62bba6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "indexed_data = [torch.tensor([vocab[token] for token in tokenizer(line)])\n",
    "                for line in df['lyrics']]\n",
    "\n",
    "# Include padding for same shape size\n",
    "max_seq_length = max(len(seq) for seq in indexed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c4dd07-d21a-422a-86a5-4cac5aa650fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = pad_sequence(indexed_data, batch_first=True, padding_value=vocab['<pad>'])\n",
    "indexed_labels = torch.tensor(label_encoder.fit_transform(df['genre']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b750e1b7-8828-4332-adec-f8468f3aff1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174,529 training samples\n",
      "43,633 validation samples\n"
     ]
    }
   ],
   "source": [
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, lyrics, genre):\n",
    "        self.lyrics = lyrics\n",
    "        self.genre = genre\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.genre)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lyrics[idx], self.genre[idx]\n",
    "\n",
    "dataset = LyricsDataset(padded_data, indexed_labels)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d009163d-5b49-4e0d-9f12-6c28dfcd127a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            sampler = SequentialSampler(val_dataset),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c225a9-57ca-4058-a64a-9c54f3a27f7b",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aee8a0a4-158d-458e-b2a4-ce272aa99334",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttLayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim):\n",
    "        super(AttLayer, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = nn.Parameter(torch.randn(input_size, hidden_dim))\n",
    "        self.bw = nn.Parameter(torch.zeros(hidden_dim))\n",
    "        self.uw = nn.Parameter(torch.randn(hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_words, hidden_size = x.size()\n",
    "        x_reshaped = x.reshape(-1, hidden_size)\n",
    "\n",
    "        ui = torch.tanh(torch.matmul(x_reshaped, self.W) + self.bw)\n",
    "        intermed = torch.sum(self.uw * ui, dim=1)\n",
    "\n",
    "        intermed = intermed.view(batch_size, num_words)\n",
    "        weights = torch.softmax(intermed, dim=-1)\n",
    "        weights = weights.unsqueeze(-1)\n",
    "\n",
    "        weighted_input = x * weights\n",
    "        return torch.sum(weighted_input, dim=1)\n",
    "\n",
    "class HAN_GRU(nn.Module):\n",
    "    def __init__(self, num_words, embedding_vector_length, hidden_size, attention_size, max_words_per_line, max_num_lines, output_size):\n",
    "        super(HAN_GRU, self).__init__()\n",
    "\n",
    "        self.word_embedding = nn.Embedding(num_words, embedding_vector_length)\n",
    "        self.word_gru = nn.GRU(embedding_vector_length, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.word_attention = AttLayer(hidden_size * 2, attention_size)\n",
    "\n",
    "        self.sentence_gru = nn.GRU(hidden_size * 2, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.sentence_attention = AttLayer(hidden_size * 2, attention_size)\n",
    "\n",
    "        self.max_words_per_line = max_words_per_line\n",
    "        self.max_num_lines = max_num_lines\n",
    "\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        word_embedded = self.word_embedding(inputs)\n",
    "\n",
    "\n",
    "        word_output, _ = self.word_gru(word_embedded)\n",
    "        word_attention_output = self.word_attention(word_output)\n",
    "\n",
    "        batch_size = word_attention_output.size(0)\n",
    "        sentence_input = word_attention_output.view(batch_size, -1, word_attention_output.size(-1))\n",
    "\n",
    "        sentence_output, _ = self.sentence_gru(sentence_input)\n",
    "        sentence_attention_output = self.sentence_attention(sentence_output)\n",
    "\n",
    "        \n",
    "        document_output = sentence_attention_output.view(batch_size, -1)\n",
    "        output = self.fc(self.dropout(document_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef71408-8e5b-4d67-826d-00474ec10357",
   "metadata": {},
   "source": [
    "Parameters changed:\n",
    "- Original attention size was 100; changed to 200\n",
    "- why? to try capture more complex relationships of the lyrics with the risk of potentially overfitting\n",
    "\n",
    "- Dropout rate from 0.3 --> 0.4\n",
    "- why? improve generalization and prevent overfitting\n",
    "\n",
    "- hidden size from 128 --> 96\n",
    "- why? Smaller hidden sizes may lead to a more regularized model that generalizes better to unseen data. It prevents the model from memorizing the training data, forcing it to learn more abstract and useful representations. --> try obtain patterns from lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f612658-3bad-4460-a45e-b535f13abaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Words Per Line: 6232\n",
      "Max Number of Lines: 759\n"
     ]
    }
   ],
   "source": [
    "# Calculate max words per line and max number of lines\n",
    "max_words_per_line = df['lyrics'].apply(lambda x: len(x.split())).max()\n",
    "max_num_lines = df['lyrics'].apply(lambda x: len(x.split('\\n'))).max()\n",
    "print(f\"Max Words Per Line: {max_words_per_line}\")\n",
    "print(f\"Max Number of Lines: {max_num_lines}\")\n",
    "\n",
    "attention_size = 200\n",
    "hidden_size = 96\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 512 \n",
    "output_size = len(df['genre'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d5fe08f-e184-462a-ace2-a72631b9f1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HAN_GRU(\n",
      "  (word_embedding): Embedding(245576, 512)\n",
      "  (word_gru): GRU(512, 96, batch_first=True, bidirectional=True)\n",
      "  (word_attention): AttLayer()\n",
      "  (sentence_gru): GRU(192, 96, batch_first=True, bidirectional=True)\n",
      "  (sentence_attention): AttLayer()\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      "  (fc): Linear(in_features=192, out_features=11, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = HAN_GRU(vocab_size, embedding_dim, hidden_size, attention_size,\n",
    "                max_words_per_line, max_num_lines, output_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "812d4fd0-1543-45f6-a929-5ed325a6f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [18:25<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Train Loss: 1.5164 Train Accuracy: 52.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [18:23<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10] Train Loss: 1.4642 Train Accuracy: 53.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [18:22<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10] Train Loss: 1.4317 Train Accuracy: 54.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [18:22<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10] Train Loss: 1.3997 Train Accuracy: 55.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [18:21<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10] Train Loss: 1.3794 Train Accuracy: 56.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [18:23<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10] Train Loss: 1.3563 Train Accuracy: 56.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [18:24<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10] Train Loss: 1.3351 Train Accuracy: 57.29%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [18:22<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10] Train Loss: 1.3124 Train Accuracy: 57.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [18:21<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10] Train Loss: 1.2893 Train Accuracy: 58.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5455/5455 [18:25<00:00,  4.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10] Train Loss: 1.2726 Train Accuracy: 59.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "losses, accuracies = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    epoch_accuracy = (correct_predictions / total_predictions) * 100\n",
    "    losses.append(epoch_loss)\n",
    "    accuracies.append(epoch_accuracy)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Train Loss: {epoch_loss:.4f} Train Accuracy: {epoch_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66000c9f-2804-4f7d-a0d6-350461a9b17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.3865, Accuracy: 55.79%\n"
     ]
    }
   ],
   "source": [
    "# Validation loop (optional)\n",
    "model.eval()\n",
    "val_running_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in validation_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        val_loss = criterion(outputs, labels)\n",
    "        val_running_loss += val_loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    avg_val_loss = val_running_loss / len(validation_dataloader)\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f203bc2-c37e-4c8a-bb50-a0912ba6f288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
